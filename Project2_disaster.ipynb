{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# ML Pipeline Preparation\nFollow the instructions below to help you create your ML pipeline.\n### 1. Import libraries and load data from database.\n- Import Python libraries\n- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n- Define feature and target variables X and Y",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# import libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sqlite3\nimport sqlalchemy\nfrom sqlalchemy import create_engine\n\nimport json\nimport plotly\nimport plotly.express as px\nimport pandas as pd\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\nfrom flask import Flask\nfrom flask import render_template, request\n\nimport joblib\nfrom sqlalchemy import create_engine\n\npd.set_option('display.max_rows', 500)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# load data from database\nengine = create_engine('sqlite:///InsertDatabaseName.db')\ndf = pd.read_sql_table('message', engine)\nX = df.message.values\nY = df.iloc[:, 4:].values",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We divide the message genres per categories",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# We take data from the fourth column. This column is 'genre', we have to do a groupby and sum\ndf_categories_genre = df.iloc[:, 4:].groupby('genre').sum().T\n\n# We transpose the df\ndf_categories_genre = df_categories_genre.T\n\nfig = go.Figure(data=[\n    go.Bar(df_categories_genre, x=df_categories_genre, y=['direct', 'news', 'social'])])\n\n# Change the bar mode\nfig.update_layout(barmode='stack')\n\n# Show the plot\nfig.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We divide the number of message per genres, without categories",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# We take the column 'genre' and we do a groupby.\ndf_messages_genre = df.groupby('genre').count()\n\nfig = px.bar(df_messages_genre, x=df_messages_genre.index, y='index', labels = {'x':'Message Genre', 'index' : '# Messages'})\n\n# Show the plot\nfig.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We divide the message per categories",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# We take data from the fith column. This column is 'related'\ndf_categories = pd.DataFrame(df.iloc[:, 5:].sum())\n\nfig = px.bar(df_categories, y='count', x=df_categories, text_auto='.2s',\n            title=\"Messages per categories\")\n            labels = {'index':' Categories', 'count' : '# Messages'}\n    \nfig.update_traces(textfont_size=12, textangle=0, textposition=\"outside\", cliponaxis=False)\n\n# Show the plot\nfig.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Now we can see the same data of the last plot, but we plot the number of categories per message, instead of message per categories",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_categories_message = pd.DataFrame()\n\n# Create a column that sum the values from the fourth column. This sum will be the x axis\ndf_categories_message['# categories'] = df.iloc[:, 4:].sum(axis=1)\n\nfig = px.bar(df_categories_message, x='# categories')\n\n# Show the plot\nfig.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### 2. Write a tokenization function to process your text data",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def tokenize(text):\n    \n    # normalize case and remove punctuation\n    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n    \n    # tokenize text\n    tokens = word_tokenize(text)\n    \n    # lower-case and lemmatize\n    lemmatizer = WordNetLemmatizer()\n    clean_tokens = []\n    for t in tokens:\n        clean_tok = lemmatizer.lemmatize(t).lower().strip()\n        clean_tokens.append(clean_tok)\n    \n    # Word that are not in the stopword that we download from NLTK. We remove it \n    if remove_stopwords:\n        clean_tokens = [word for word in clean_tokens if word not in stop_words] \n        \n    return clean_tokens  \n    #pass",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### 3. Build a machine learning pipeline\nThis machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "pipeline = [('vect', CountVectorizer(tokenizer=tokenize)),\n            ('tfidf', TfidfTransformer()),\n            ('clf', MultiOutputClassifier(RandomForestClassifier()))]",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### 4. Train pipeline\n- Split data into train and test sets\n- Train pipeline",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Split data into train and test sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Train pipeline\npipeline.fit(X_train, Y_train)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### 5. Test your model\nReport the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "Y_pred = pipeline.predict(X_test)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# We define the labels. From the fifth column, that is 'related'\nlabel = df.iloc[:, 5:].columns.values\n\n# The target names display names matching the labels in the same order. We return output as dict\nreport = classification_report(Y_test, Y_pred, target_names=label, output_dict = True)\n\n# Show the result\n#display(pd.DataFrame(report))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We need to do a comparison dataframe that we use and will use in the next step of the model",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "comparison_df = (pd.DataFrame(report))['weighted avg']\ncomparison_df = pd.DataFrame(comparison_df).T.rename({'weighted avg':'RandomForestClassifier (default)'})\ncomparison_df",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### 6. Improve your model\nUse grid search to find better parameters. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# We define the parameters\nparam_grid = [\n    {   \"estimator_min_samples_leaf\" : [1, 3, 12],\n        \"estimator_min_samples_split\" : [3, 6],\n        \"estimator_max_depth\": [None, 3, 12],\n        \"estimator_max_features\" : [None, 'sqrt','log2']}\n    ]\n\n\ncv = GridSearchCV(default_pipeline, param_grid, cv =5, scoring=['f1_weight', 'precision_weight', 'recall_weight'],verbose=0, refit='f1_weighted', return_train_score=True, \n                  n_jobs=-1)\n\nx = X_train\ny = Y_train\n\ncv.fit(x , y)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### 7. Test your model\nShow the accuracy, precision, and recall of the tuned model.  \n\nSince this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Predict on the data\nY_pred = cv.predict(X_test)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# We define the labels. From the fifth column, that is 'related'\nlabel = df.iloc[:, 5:].columns.values\n\n# The target names display names matching the labels in the same order. We return output as dict\ncv_report = classification_report(Y_test, Y_pred, target_names=label, output_dict = True)\n\n# Show the result\n#display(pd.DataFrame(cv_report))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# We show the result\n\nparam_name = {\n    'estimator__min_samples_leaf_param': 'min_samples_leaf',\n    'estimator__min_samples_split_param': 'min_samples_split',\n    'estimator__max_depth_param' : 'max_depth', \n    'estimator__max_features_param': 'max_features',\n    'test_f1_weight' : 'test_f1_weight',\n    'test_precision_weight' : 'test_precision_weight',\n    'test_recall_weight' : 'test_recall_weight'\n    }\n\nresult = pd.DataFrame.from_dict(cv.result)\n\nresult_df = result[param_name.keys()]\n\nresult_df = result_df.rename(columns = param_name).sort_values('test_f1_weight')\n\nresult_df",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# We have to save the best parameters\ncv_best_parameters = cv.best_params_\ncv_best_parameters",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# We add the resultes row to the comparison df\nrow_cv = pd.DataFrame((pd.DataFrame(cv_report))['weighted avg']).T\ncomparison_df = pd.concat([comparison_df, row_cv]).rename({'weighted avg':'RandomForestClassifier (GridSearchCV)'})\ncomparison_df",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### 8. Try improving your model further. Here are a few ideas:\n* try other machine learning algorithms\n* add other features besides the TF-IDF",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "One iof the mmost commom use in the machine learning is the",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### 9. Export your model as a pickle file",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### 10. Use this notebook to complete `train.py`\nUse the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}