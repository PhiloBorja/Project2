{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# ML Pipeline Preparation\nFollow the instructions below to help you create your ML pipeline.\n### 1. Import libraries and load data from database.\n- Import Python libraries\n- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n- Define feature and target variables X and Y",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# import libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sqlite3\nimport sqlalchemy\nfrom sqlalchemy import create_engine\n\nimport json\nimport plotly\nimport plotly.express as px\nimport pandas as pd\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\nfrom flask import Flask\nfrom flask import render_template, request\n\nimport joblib\nfrom sqlalchemy import create_engine\n\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\n\n\npd.set_option('display.max_rows', 500)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "ename": "<class 'ModuleNotFoundError'>",
          "evalue": "No module named 'plotly'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_engine\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "# load data from database\nengine = create_engine('sqlite:///DisasterResponse.db')\ndf = pd.read_sql_table('message', engine)\nX = df.message.values\nY = df.iloc[:, 4:].values",
      "metadata": {
        "trusted": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "ename": "<class 'ValueError'>",
          "evalue": "Table message not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load data from database\u001b[39;00m\n\u001b[1;32m      2\u001b[0m engine \u001b[38;5;241m=\u001b[39m create_engine(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqlite:///InsertDatabaseName.db\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      5\u001b[0m Y \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m4\u001b[39m:]\u001b[38;5;241m.\u001b[39mvalues\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/pandas/io/sql.py:282\u001b[0m, in \u001b[0;36mread_sql_table\u001b[0;34m(table_name, con, schema, index_col, coerce_float, parse_dates, columns, chunksize)\u001b[0m\n\u001b[1;32m    280\u001b[0m pandas_sql \u001b[38;5;241m=\u001b[39m pandasSQL_builder(con, schema\u001b[38;5;241m=\u001b[39mschema)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mhas_table(table_name):\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# error: Item \"SQLiteDatabase\" of \"Union[SQLDatabase, SQLiteDatabase]\"\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# has no attribute \"read_table\"\u001b[39;00m\n\u001b[1;32m    286\u001b[0m table \u001b[38;5;241m=\u001b[39m pandas_sql\u001b[38;5;241m.\u001b[39mread_table(  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     table_name,\n\u001b[1;32m    288\u001b[0m     index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    292\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[1;32m    293\u001b[0m )\n",
            "\u001b[0;31mValueError\u001b[0m: Table message not found"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "We divide the message genres per categories",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# We take data from the fourth column. This column is 'genre', we have to do a groupby and sum\ndf_categories_genre = df.iloc[:, 4:].groupby('genre').sum().T\n\n# We transpose the df\ndf_categories_genre = df_categories_genre.T\n\nfig = go.Figure(data=[\n    go.Bar(df_categories_genre, x=df_categories_genre, y=['direct', 'news', 'social'])])\n\n# Change the bar mode\nfig.update_layout(barmode='stack')\n\n# Show the plot\nfig.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We divide the number of message per genres, without categories",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# We take the column 'genre' and we do a groupby.\ndf_messages_genre = df.groupby('genre').count()\n\nfig = px.bar(df_messages_genre, x=df_messages_genre.index, y='index', labels = {'x':'Message Genre', 'index' : '# Messages'})\n\n# Show the plot\nfig.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We divide the message per categories",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# We take data from the fith column. This column is 'related'\ndf_categories = pd.DataFrame(df.iloc[:, 5:].sum())\n\nfig = px.bar(df_categories, y='count', x=df_categories, text_auto='.2s',\n            title=\"Messages per categories\")\n            labels = {'index':' Categories', 'count' : '# Messages'}\n    \nfig.update_traces(textfont_size=12, textangle=0, textposition=\"outside\", cliponaxis=False)\n\n# Show the plot\nfig.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Now we can see the same data of the last plot, but we plot the number of categories per message, instead of message per categories",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_categories_message = pd.DataFrame()\n\n# Create a column that sum the values from the fourth column. This sum will be the x axis\ndf_categories_message['# categories'] = df.iloc[:, 4:].sum(axis=1)\n\nfig = px.bar(df_categories_message, x='# categories')\n\n# Show the plot\nfig.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### 2. Write a tokenization function to process your text data",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def tokenize(text):\n    \n    # normalize case and remove punctuation\n    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n    \n    # tokenize text\n    tokens = word_tokenize(text)\n    \n    # lower-case and lemmatize\n    lemmatizer = WordNetLemmatizer()\n    clean_tokens = []\n    for t in tokens:\n        clean_tok = lemmatizer.lemmatize(t).lower().strip()\n        clean_tokens.append(clean_tok)\n    \n    # Word that are not in the stopword that we download from NLTK. We remove it \n    if remove_stopwords:\n        clean_tokens = [word for word in clean_tokens if word not in stop_words] \n        \n    return clean_tokens  \n    #pass",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### 3. Build a machine learning pipeline\nThis machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "pipeline = [('vect', CountVectorizer(tokenizer=tokenize)),\n            ('tfidf', TfidfTransformer()),\n            ('clf', MultiOutputClassifier(RandomForestClassifier()))]",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### 4. Train pipeline\n- Split data into train and test sets\n- Train pipeline",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Split data into train and test sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Train pipeline\npipeline.fit(X_train, Y_train)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### 5. Test your model\nReport the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "Y_pred = pipeline.predict(X_test)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# We define the labels. From the fifth column, that is 'related'\nlabel = df.iloc[:, 5:].columns.values\n\n# The target names display names matching the labels in the same order. We return output as dict\nreport = classification_report(Y_test, Y_pred, target_names=label, output_dict = True)\n\n# Show the result\n#display(pd.DataFrame(report))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We need to do a comparison dataframe that we use and will use in the next step of the model",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "comparison_df = (pd.DataFrame(report))['weighted avg']\ncomparison_df = pd.DataFrame(comparison_df).T.rename({'weighted avg':'RandomForestClassifier (default)'})\ncomparison_df",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### 6. Improve your model\nUse grid search to find better parameters. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# We define the parameters\nparam_grid = [\n    {   \"estimator_min_samples_leaf\" : [1, 3, 12],\n        \"estimator_min_samples_split\" : [3, 6],\n        \"estimator_max_depth\": [None, 3, 12],\n        \"estimator_max_features\" : [None, 'sqrt','log2']}\n    ]\n\n\ncv = GridSearchCV(default_pipeline, param_grid, cv =5, scoring=['f1_weight', 'precision_weight', 'recall_weight'],verbose=0, refit='f1_weighted', return_train_score=True, \n                  n_jobs=-1)\n\nx = X_train\ny = Y_train\n\ncv.fit(x , y)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### 7. Test your model\nShow the accuracy, precision, and recall of the tuned model.  \n\nSince this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Predict on the data\nY_pred = cv.predict(X_test)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# We define the labels. From the fifth column, that is 'related'\nlabel = df.iloc[:, 5:].columns.values\n\n# The target names display names matching the labels in the same order. We return output as dict\ncv_report = classification_report(Y_test, Y_pred, target_names=label, output_dict = True)\n\n# Show the result\n#display(pd.DataFrame(cv_report))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# We show the result\n\nparam_name = {\n    'estimator__min_samples_leaf_param': 'min_samples_leaf',\n    'estimator__min_samples_split_param': 'min_samples_split',\n    'estimator__max_depth_param' : 'max_depth', \n    'estimator__max_features_param': 'max_features',\n    'test_f1_weight' : 'test_f1_weight',\n    'test_precision_weight' : 'test_precision_weight',\n    'test_recall_weight' : 'test_recall_weight'\n    }\n\nresult = pd.DataFrame.from_dict(cv.result)\n\nresult_df = result[param_name.keys()]\n\nresult_df = result_df.rename(columns = param_name).sort_values('test_f1_weight')\n\nresult_df",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# We have to save the best parameters\ncv_best_parameters = cv.best_params_\ncv_best_parameters",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# We add the resultes row to the comparison df\nrow_cv = pd.DataFrame((pd.DataFrame(cv_report))['weighted avg']).T\ncomparison_df = pd.concat([comparison_df, row_cv]).rename({'weighted avg':'RandomForestClassifier (GridSearchCV)'})\ncomparison_df",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### 8. Try improving your model further. Here are a few ideas:\n* try other machine learning algorithms\n* add other features besides the TF-IDF",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "We can try different classifier. We used the MultiOutputClassifier, but now we use LinearSVC",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "pipeline_svc = Pipeline([\n    ('vect', CountVectorizer(tokenizer=tokenize)),\n    ('tfidf', TfidfTransformer()),\n    ('clf', OneVsRestClassifier(LinearSVC()))])\n\npipeline_svc.fit(X_train, Y_train)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "Y_pred = pipeline_svc.predict(X_test)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# We define the labels. From the fifth column, that is 'related'\nlabels = df.iloc[:, 5:].columns.values \n\n# The target names display names matching the labels in the same order. We return output as dict\nsvc_report = classification_report(Y_test, Y_pred, target_names=labels, output_dict = True)\n\n# Show the result\ndisplay(pd.DataFrame(svc_report))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# We add the result to comparison df\nrow_svc = pd.DataFrame((pd.DataFrame(svc_report))['weighted avg']).T\n\ndf_comparison = pd.concat([df_comparison, row_svc]).rename({'weighted avg':'Linear SVC (default)'})\n\ndf_comparison",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### 9. Export your model as a pickle file",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "model_name = 'RandomForestClassifier.pkl'",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Save pickle file\nwith open(model_name, 'wb') as handle:\n    pickle.dump(cv, handle, protocol=pickle.HIGHEST_PROTOCOL)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Load from pickle file\nwith open(model_name, 'rb') as handle:\n    cv_pickle = pickle.load(handle)\nprint(cv_pickle)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### 10. Use this notebook to complete `train.py`\nUse the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}